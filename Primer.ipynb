{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Primer Notebook\n",
    "### By Kevin Chamberlin\n",
    "__Updated July 2024__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing codespace requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fsspec --quiet\n",
    "%pip install s3fs --quiet\n",
    "%pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/02 15:36:56 WARN Utils: Your hostname, codespaces-00e44c resolves to a loopback address: 127.0.0.1; using 10.0.0.55 instead (on interface eth0)\n",
      "24/08/02 15:36:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/02 15:36:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing dataframes\n",
    "This is also commonly completed by reading data from a datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: bigint, FirstName: string, LastName: string, Hometown: string, Favorite Color: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+----------------+--------------+\n",
      "| ID|FirstName|LastName|        Hometown|Favorite Color|\n",
      "+---+---------+--------+----------------+--------------+\n",
      "|  1|    Steve|  Rogers|    Brooklyn, NY|          Blue|\n",
      "|  2|     Tony|   Stark|  Manahattan, NY|          Gold|\n",
      "|  3|    Peter|  Parker|      Queens, NY|          Blue|\n",
      "|  4|    Scott|    Lang|Coral Gables, FL|          Blue|\n",
      "|  5|  Natasha|Romanoff|Stalingrad, USSR|         Black|\n",
      "|  6|    Clint|  Barton|     Waverly, IA|        Purple|\n",
      "+---+---------+--------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of manually creating a Spark DataFrame using PySpark code \n",
    "l_avengers_data = [[1, \"Steve\", \"Rogers\", \"Brooklyn, NY\", \"Blue\"]\n",
    "              ,[2, \"Tony\", \"Stark\", \"Manahattan, NY\", \"Gold\"]\n",
    "              ,[3, \"Peter\", \"Parker\", \"Queens, NY\", \"Blue\"]\n",
    "              ,[4, \"Scott\", \"Lang\", \"Coral Gables, FL\", \"Blue\"]\n",
    "              ,[5, \"Natasha\", \"Romanoff\", \"Stalingrad, USSR\", \"Black\"]\n",
    "              ,[6, \"Clint\", \"Barton\", \"Waverly, IA\", \"Purple\"]]\n",
    "\n",
    "l_avengers_col_names = [\"ID\", \"FirstName\", \"LastName\", \"Hometown\", \"Favorite Color\"]\n",
    "\n",
    "sdf_avengers = spark.createDataFrame(l_avengers_data, l_avengers_col_names)\n",
    "\n",
    "# display() can be used in a platform like Databricks to display data in an interactive format\n",
    "display(sdf_avengers) \n",
    "\n",
    "# show() is used here because GitHub codespaces do not support display()\n",
    "sdf_avengers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "#from pyspark.sql.functions import col, udf, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are examples of commonly used libraries and how to import them. If you are familiar with Python, this formatting will look very familiar. When we talk about Pyspark we are frequently referring to a set of SQL functions that have been written in Python to be used on a distributed computing platform like Databricks!\n",
    "I suggest starting all Databricks notebooks with the command import pyspark.sql.functions as F. I prefer this notation over the potentially simplier from pyspark.sql.functions import * because the former highlights more tracability for troubleshooting and prevents any potential conflicts with function names. This conflict has appeared a handful of times in our main product code, so it's best to just use \"F.\" notation to save integration time later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| ID|        FullName|\n",
      "+---+----------------+\n",
      "|  1|    Steve Rogers|\n",
      "|  2|      Tony Stark|\n",
      "|  3|    Peter Parker|\n",
      "|  4|      Scott Lang|\n",
      "|  5|Natasha Romanoff|\n",
      "|  6|    Clint Barton|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic PySpark SQL Functions: .withColumn() and .select()\n",
    "\n",
    "sdf_avengers_names = (sdf_avengers\n",
    "                      .withColumn(\"FullName\", F.concat(F.col(\"FirstName\"), F.lit(\" \"), F.col(\"LastName\")))\n",
    "                      .select(F.col(\"ID\"), F.col(\"FullName\"))\n",
    "                     )\n",
    "\n",
    "# display(sdf_avengers_names)\n",
    "sdf_avengers_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| ID|           Hero|\n",
      "+---+---------------+\n",
      "|  1|Captain America|\n",
      "|  2|       Iron Man|\n",
      "|  3|      Spiderman|\n",
      "|  4|        Ant-Man|\n",
      "|  5|    Black Widow|\n",
      "|  6|        Hawkeye|\n",
      "+---+---------------+\n",
      "\n",
      "+---+---------+--------+--------+--------------+\n",
      "| ID|FirstName|LastName|Hometown|Favorite Color|\n",
      "+---+---------+--------+--------+--------------+\n",
      "|  7|    Wanda|Maximoff| Sokovia|       Scarlet|\n",
      "+---+---------+--------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe completely from scratch\n",
    "\n",
    "l_hero_data = [[1, \"Captain America\"]\n",
    "              ,[2, \"Iron Man\"]\n",
    "              ,[3, \"Spiderman\"]\n",
    "              ,[4, \"Ant-Man\"]\n",
    "              ,[5, \"Black Widow\"]\n",
    "              ,[6, \"Hawkeye\"]]\n",
    "\n",
    "l_hero_col_names = [\"ID\", \"Hero\"]\n",
    "\n",
    "sdf_avengers_heroes = spark.createDataFrame(l_hero_data, l_hero_col_names)\n",
    "\n",
    "\n",
    "# Create a new dataframe that matches the columns of an existing dataframe\n",
    "\n",
    "l_new_avenger_data = [[7, \"Wanda\", \"Maximoff\", \"Sokovia\", \"Scarlet\"]]\n",
    "\n",
    "l_new_avenger_col_names = sdf_avengers.columns # data and metadata from DFs can be called upon\n",
    "\n",
    "sdf_avengers_new = spark.createDataFrame(l_new_avenger_data, l_new_avenger_col_names)\n",
    "\n",
    "# Display both new dataframes\n",
    "# display(sdf_avengers_heroes)\n",
    "sdf_avengers_heroes.show()\n",
    "\n",
    "# display(sdf_avengers_new)\n",
    "sdf_avengers_new.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+----------------+--------------+---------------+\n",
      "| ID|FirstName|LastName|        Hometown|Favorite Color|           Hero|\n",
      "+---+---------+--------+----------------+--------------+---------------+\n",
      "|  1|    Steve|  Rogers|    Brooklyn, NY|          Blue|Captain America|\n",
      "|  3|    Peter|  Parker|      Queens, NY|          Blue|      Spiderman|\n",
      "|  2|     Tony|   Stark|  Manahattan, NY|          Gold|       Iron Man|\n",
      "|  6|    Clint|  Barton|     Waverly, IA|        Purple|        Hawkeye|\n",
      "|  5|  Natasha|Romanoff|Stalingrad, USSR|         Black|    Black Widow|\n",
      "|  4|    Scott|    Lang|Coral Gables, FL|          Blue|        Ant-Man|\n",
      "|  7|    Wanda|Maximoff|         Sokovia|       Scarlet|           NULL|\n",
      "+---+---------+--------+----------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Two ways of combining data\n",
    "sdf_avengers_expanded = (sdf_avengers\n",
    "                         .union(sdf_avengers_new)\n",
    "                         .join(sdf_avengers_heroes, on = \"ID\", how = 'left')\n",
    "                        )\n",
    "# NOTE: If a table you're joing with is relatively small a \"broadcast join\" may improve processing time while distributed\n",
    "\n",
    "# display(sdf_avengers_expanded)\n",
    "sdf_avengers_expanded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------------+---------------+-------------+\n",
      "|FirstInitial|LastName|        Hometown|           Hero|FavoriteColor|\n",
      "+------------+--------+----------------+---------------+-------------+\n",
      "|           S|  Rogers|    Brooklyn, NY|Captain America|         Blue|\n",
      "|           P|  Parker|      Queens, NY|      Spiderman|         Blue|\n",
      "|           S|    Lang|Coral Gables, FL|        Ant-Man|         Blue|\n",
      "|           N|Romanoff|Stalingrad, USSR|    Black Widow|        Black|\n",
      "|           C|  Barton|     Waverly, IA|        Hawkeye|       Purple|\n",
      "+------------+--------+----------------+---------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtering is generally a good skill to be able to utilize\n",
    "\n",
    "sdf_avengers_filtered = (sdf_avengers_expanded\n",
    "                         .filter(F.col(\"Hero\").isNotNull())\n",
    "                         .filter(F.col(\"Favorite Color\") != \"Gold\")\n",
    "                         .withColumn(\"FirstInitial\", F.substring(\"FirstName\", 1,1))\n",
    "                         .drop(\"FirstName\")\n",
    "                         .select(\"FirstInitial\", \"LastName\", \"Hometown\", \"Hero\", F.col(\"Favorite Color\").alias(\"FavoriteColor\"))\n",
    "                        )\n",
    "\n",
    "# display(sdf_avengers_filtered)\n",
    "sdf_avengers_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Cloud Computing\n",
    "At its core, Databricks is a platform for doing cloud computing. That's why you run notebooks on a \"Cluster\" -- it's a grouping of workers and a driver that process your code.\n",
    "When you run code distributed across the worker nodes, it's very fast until the results are collected by the driver node. This notably happens in any displays, graphing, or\n",
    "when you convert out of the Spark environment. This is why you install the PySpark SQL functions and use Spark DataFrames instead of something like Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, FirstInitial: string, LastName: string, Hometown: string, Hero: string, FavoriteColor: string]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at what your environment classifies sdf_avengers_filtered as\n",
    "\n",
    "print(sdf_avengers_filtered.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Tasks\n",
    "Below are some examples of common tasks that might need to be done on data in Databricks. I'll add to this section of this primer notebook as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data stucture using the \"Data Profile\" option on display() output (select the \"+\" button next to the word \"Table\" on the output of this cell)\n",
    "# This is a unique feature of Databricks and may not be useful in all contexts.\n",
    "\n",
    "# display(sdf_avengers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|FavoriteColor|TotalWeightLbs|\n",
      "+-------------+--------------+\n",
      "|       Purple|           200|\n",
      "|         Blue|           600|\n",
      "|        Black|           200|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aggregate sums of columns with a groupBy() on other columns\n",
    "\n",
    "sdf_avengers_weights = (sdf_avengers_filtered\n",
    "                        .withColumn('WeightLbs', F.lit(200)) # in this example, each avenger weighs 200 lbs for simplicity\n",
    "                       )\n",
    "\n",
    "sdf_avengers_sum = (sdf_avengers_weights\n",
    "                    .groupBy('FavoriteColor')\n",
    "                    .agg(F.sum(F.col('WeightLbs')).alias('TotalWeightLbs')) # the alias method is attached to the F.sum() function to rename the output of F.sum()\n",
    "                   )\n",
    "\n",
    "# display(sdf_avengers_sum)\n",
    "sdf_avengers_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete list of functions within PySpark can be found here: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions\n",
    "As with most SQL operations, windowing can dramatically improve your code's performance. I've found them to b the solution to a lot of\n",
    "headaches with compute limitations when using PySpark. Below are some simple syntax examples of what might be useful to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/02 15:37:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/08/02 15:37:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/02 15:37:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------------+---------------+-------------+---------+-----------------+\n",
      "|FirstInitial|LastName|        Hometown|           Hero|FavoriteColor|WeightLbs|ColorSumWeightLbs|\n",
      "+------------+--------+----------------+---------------+-------------+---------+-----------------+\n",
      "|           N|Romanoff|Stalingrad, USSR|    Black Widow|        Black|      200|              200|\n",
      "|           S|  Rogers|    Brooklyn, NY|Captain America|         Blue|      200|              600|\n",
      "|           P|  Parker|      Queens, NY|      Spiderman|         Blue|      200|              600|\n",
      "|           S|    Lang|Coral Gables, FL|        Ant-Man|         Blue|      200|              600|\n",
      "|           C|  Barton|     Waverly, IA|        Hawkeye|       Purple|      200|              200|\n",
      "+------------+--------+----------------+---------------+-------------+---------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/02 15:37:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/02 15:37:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/02 15:37:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/02 15:37:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/02 15:37:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/02 15:37:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------------+---------------+-------------+---------+--------------------+\n",
      "|FirstInitial|LastName|        Hometown|           Hero|FavoriteColor|AlphaRank|AlphaRankWithinColor|\n",
      "+------------+--------+----------------+---------------+-------------+---------+--------------------+\n",
      "|           N|Romanoff|Stalingrad, USSR|    Black Widow|        Black|        5|                   1|\n",
      "|           S|    Lang|Coral Gables, FL|        Ant-Man|         Blue|        2|                   1|\n",
      "|           P|  Parker|      Queens, NY|      Spiderman|         Blue|        3|                   2|\n",
      "|           S|  Rogers|    Brooklyn, NY|Captain America|         Blue|        4|                   3|\n",
      "|           C|  Barton|     Waverly, IA|        Hawkeye|       Purple|        1|                   1|\n",
      "+------------+--------+----------------+---------------+-------------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importing necessary class\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "# Partitioning within the data\n",
    "sdf_avengers_partition = (sdf_avengers_weights\n",
    "                          .withColumn(\"ColorSumWeightLbs\", \n",
    "                                      F.sum(\"WeightLbs\").over(W.partitionBy([\"FavoriteColor\"]))\n",
    "                                      )\n",
    "                          )\n",
    "# display(sdf_avengers_partition)\n",
    "sdf_avengers_partition.show()\n",
    "\n",
    "\n",
    "\n",
    "# Ordering data\n",
    "sdf_avengers_ordered = (sdf_avengers_filtered\n",
    "                        .withColumn(\"AlphaRank\",\n",
    "                                    F.rank().over(W.orderBy(\"LastName\")) # NOTE: The parameter for these Window functions can be a string or list of strings\n",
    "                                    )\n",
    "                        .withColumn(\"AlphaRankWithinColor\",\n",
    "                                    F.rank().over(W.partitionBy(\"FavoriteColor\").orderBy(\"LastName\"))\n",
    "                                    )\n",
    "                        )\n",
    "# display(sdf_avengers_ordered)\n",
    "sdf_avengers_ordered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databricks Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Workspace__ is the term used to describe the file structure around notebooks in Databricks. They're generally organized by the original author of the notebooks, but companies can handle this differently if they wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Repos__ or __Repositories__ reference mostly Github or Azure DevOps repositories of code. This is a way to maintain version controlling and organized development/implementation of code. In Databricks, they are stored as a type of \"Workspace\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Catalog__ is the term used in Databricks for the database storage accessible natively within Databricks. In this notebook, I use a generic example Spark Dataframe created manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Uploading / downloading notebooks__ is fairly straightforward. If you've ever used Jupyter Notebooks within Python before, .IPYNB files are one of the primary ways to download a notebook. This option is within the \"File\" menu visible when viewing a notebook.\n",
    "To upload, right-click within the intended workspace, and selected \"Import\". From there you can drag-and-drop, or select a number of file types to be used within Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cloning__ is a key feature of Databricks file management. You can essentially think of it as a Copy/Paste for the entire notebook. This can be usefully when you want to perhaps make edits to someone else's notebook you don't have\n",
    "edit permissions for, or when duplicating large sections of code in one of your own previously developed notebooks. Try cloning this notebook to your own workspace now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Workflows/Jobs__ are something I used extensively in previous roles to automated when Notebooks are run in the case of regular data pipelining operations. They are viewable on the \"Workflows\" tab on the left side of the screen, and new Workflows for a specific notebook\n",
    "can easily be made with the \"Schedule\" button at the top right corner of the screen while viewing a notebook. There are some nuances to workflows and the clusters they use, but we can go deeper into that at a later date if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute__ is the source of setting up and monitoring clusters within Databricks. For most users, monitoring is only necessary when bottlenecking is occuring. Data engineers may use this information for optimizing pipeline functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Additional resources from Databricks:_ https://docs.databricks.com/notebooks/notebooks-use.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
